{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Обработка web-страниц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Скачивание web-страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(\"https://ru.wikipedia.org/wiki/Python\").read().decode('utf-8')\n",
    "print(html.count('C++'))\n",
    "print(urlopen(\"https://ru.wikipedia.org/wiki/Python\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "html = urlopen(\"https://stepik.org/media/attachments/lesson/209717/1.html\").read().decode('utf-8')\n",
    "print('C++' if html.count('C++') > html.count('Python') else 'Python')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__#2__ C регулярками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['С++', 'С++', 'С++', 'C++', 'C++', 'C++', 'C++', 'С++', 'С++', 'С++', 'С++', 'С++', 'С++', 'С++', 'С++', 'C++', 'C++', 'C++', 'C++', 'С++', 'С++', 'C++', 'C++', 'C++', 'Си++', 'C++', 'C++', 'C++', 'C++', 'C++', 'C++'] 31\n",
      "C++\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "html = urlopen(\"https://stepik.org/media/attachments/lesson/209717/1.html\").read().decode('utf-8')\n",
    "m = re.findall(r'([CС]\\+\\+|СИ\\+\\+)', html, flags=re.IGNORECASE)\n",
    "print(m, len(m))\n",
    "print('C++' if len(m) > html.count('Python') else 'Python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function findall in module re:\n",
      "\n",
      "findall(pattern, string, flags=0)\n",
      "    Return a list of all non-overlapping matches in the string.\n",
      "    \n",
      "    If one or more capturing groups are present in the pattern, return\n",
      "    a list of groups; this will be a list of tuples if the pattern\n",
      "    has more than one group.\n",
      "    \n",
      "    Empty matches are included in the result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.findall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Обработка html как текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567  5  8 fgh\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(\"https://ru.wikipedia.org/wiki/Python\").read().decode('utf-8')\n",
    "# html = '567<fghj> <fghj> 5 < 8 fgh' # 567  5  ??? Корректно ли ???\n",
    "# html = '567<fghj> <fghj> 5 > 8 fgh' # 567  5  8 fgh\n",
    "#html = '567<fg <111 hj> <fghj> 5 > 8 fgh'   # 567  5  8 fgh ???\n",
    "#html = '567<fg <111> hj> <fghj> 5 > 8 fgh' # 567 hj  5  8 fgh ???\n",
    "\n",
    "\n",
    "text = []\n",
    "state = 0\n",
    "for c in html:\n",
    "    if c == '<':\n",
    "        state = 1\n",
    "    elif c == '>':\n",
    "        state = 0\n",
    "    elif state == 0:\n",
    "        text.append(c)\n",
    "\n",
    "text = ''.join(text)\n",
    "print(text)\n",
    "print(text.count('C++'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробую решить задачу из урока с помощью __регулярных выражений__:\n",
    "\n",
    "Добавление + в регулярку делает её нежадной.  \n",
    "По умолчанию символ \\n конца строки не подходит под точку.  \n",
    "С флагом re.DOTALL точка — вообще любой символ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text2 = re.sub(r'<.+?>', '', html, flags=re.DOTALL)\n",
    "with open('python_wiki.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "97752 97752\n",
      "0 0\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(text == text2)\n",
    "print(len(text), len(text2))\n",
    "print(text.count('<'), text2.count('<'))\n",
    "print(text.count('>'), text2.count('>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окончательное решение с помощью регулярных выражений:\n",
    "\n",
    "Плохо работает со случаями вложенных тегов и свободных < и >. Впрочем, может это и неплохо. Главное, что хорошо работает с корректной html страницей, т.к. если страница некорректна, то тогда слишком много неопределенности: какой текст нужно анализировать, а какой нужно отбрасывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567  5 < 8 fgh\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(\"https://ru.wikipedia.org/wiki/Python\").read().decode('utf-8')\n",
    "#html = '567<fghj> <fghj> 5 < 8 fgh' # 567  5 < 8 fgh\n",
    "#html = '567<fghj> <fghj> 5 > 8 fgh' # 567  5 > 8 fgh\n",
    "#html = '567<fg <111 hj> <fghj> 5 > 8 fgh'   # 567  5 > 8 fgh ???\n",
    "#html = '567<fg <111> hj> <fghj> 5 > 8 fgh' # 567 hj>  5 > 8 fgh ???\n",
    "\n",
    "\n",
    "text = re.sub(r'<.+?>', '', html, flags=re.DOTALL)\n",
    "print(text)\n",
    "print(text.count('C++'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного переделала скрипт из урока. Работает, во-первых, не совсем так, как исходный скрипт, во-вторых, неидеально, если в тексте есть свободные занки < и > (ну, у исходного скрипта, в принципе такая же проблема) (см. тест. случаи). Потенциально должен быть быстрее на больших текстах за счет уменьшения количества аппендов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567  8 fgh\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "#html = urlopen(\"https://ru.wikipedia.org/wiki/Python\").read().decode('utf-8')\n",
    "#html = '567<fghj> <fghj> 5 < 8 fgh' # 567  5  5 < 8 fgh\n",
    "html = '567<fghj> <fghj> 5 > 8 fgh'  # 567  8 fgh ??? Корректно ли ???\n",
    "\n",
    "text = []\n",
    "start = 0\n",
    "for i, c in enumerate(html):\n",
    "    if c == '<':\n",
    "        text.append(html[start:i])\n",
    "    elif c == '>':\n",
    "        state = 0\n",
    "        start = i + 1\n",
    "text.append(html[start:])\n",
    "       \n",
    "text = ''.join(text)\n",
    "print(text)\n",
    "print(text.count('C++'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного доработала свой скрипт: теперь отрабатывает хорошо, когда в тексте есть свободные < и >. Попробовала ещё на одном тестовом случае: вложенные теги (последний тестовый случай). Не работает. Ну и черт с ним. Вложенные теги по идее и не должны встречаться, впрочем, свободные < и > тоже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567<fg   5 > 8 fgh\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "#html = urlopen(\"https://ru.wikipedia.org/wiki/Python\").read().decode('utf-8')\n",
    "# html = '567<fghj> <fghj> 5 < 8 fgh'  # 567  5 < 8 fgh\n",
    "# html = '567<fghj> <fghj> 5 > 8 fgh'  # 567  5 > 8 fgh\n",
    "html = '567<fg <111 hj> <fghj> 5 > 8 fgh' # 567<fg   5 > 8 fgh\n",
    "html = '567<fg <111> hj> <fghj> 5 > 8 fgh' # 567<fg  hj>  5 > 8 fgh ???\n",
    "\n",
    "text = []\n",
    "state = 0\n",
    "start = 0\n",
    "for i, c in enumerate(html):\n",
    "    if c == '<':\n",
    "        text.append(html[start:i])\n",
    "        state = 1\n",
    "        start = i\n",
    "    elif c == '>':\n",
    "        if state:\n",
    "            start = i + 1\n",
    "        state = 0\n",
    "text.append(html[start:])\n",
    "       \n",
    "text = ''.join(text)\n",
    "print(text)\n",
    "print(text.count('C++'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл https://stepik.org/media/attachments/lesson/209719/2.html содержит статью с Википедии про язык Python. В этой статье есть теги code, которыми выделяются конструкции на языке Python. Вам нужно найти все строки, содержащиеся между тегами <code> и </code> и найти те строки, которые встречаются чаще всего и вывести их в алфавитном порядке, разделяя пробелами.\n",
    "\n",
    "Например, если исходный текст страницы выглядел бы так:\n",
    "\n",
    "<code>a</code>  \n",
    "<a>bracadabr</a>  \n",
    "<code>c</code>  \n",
    "<code>b</code>  \n",
    "<code>b</code>  \n",
    "<code>c</code>  \n",
    "то в ответ надо было бы ввести строку \"b c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "else except finally\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "html = urlopen(\"https://stepik.org/media/attachments/lesson/209719/2.html\").read().decode('utf-8')\n",
    "sorted_counters = Counter(re.findall(r'<code>(.*?)<\\/code>', html)).most_common()\n",
    "print(' '.join(sorted(s for s, c in sorted_counters if c == sorted_counters[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почти тоже самое, но без предварительной сортировки содержимого словаря, которая по идее, происходит в методе most_common():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "else except finally\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from urllib.request import urlopen\n",
    "from operator import itemgetter\n",
    "import re\n",
    "\n",
    "html = urlopen(\"https://stepik.org/media/attachments/lesson/209719/2.html\").read().decode('utf-8')\n",
    "counters = Counter(re.findall(r'<code>(.*?)<\\/code>', html))\n",
    "print(' '.join(sorted(s for s, c in counters.items() if c == max(counters.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function update in module collections:\n",
      "\n",
      "update(*args, **kwds)\n",
      "    Like dict.update() but add counts instead of replacing them.\n",
      "    \n",
      "    Source can be an iterable, a dictionary, or another Counter instance.\n",
      "    \n",
      "    >>> c = Counter('which')\n",
      "    >>> c.update('witch')           # add elements from another iterable\n",
      "    >>> d = Counter('watch')\n",
      "    >>> c.update(d)                 # add elements from another counter\n",
      "    >>> c['h']                      # four 'h' in which, witch, and watch\n",
      "    4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Counter.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "update(...)\n",
      "    D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "    If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "    In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dict.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function most_common in module collections:\n",
      "\n",
      "most_common(self, n=None)\n",
      "    List the n most common elements and their counts from the most\n",
      "    common to the least.  If n is None, then list all element counts.\n",
      "    \n",
      "    >>> Counter('abcdeabcdabcaba').most_common(3)\n",
      "    [('a', 5), ('b', 4), ('c', 3)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Counter.most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Знакомство с BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша задача:  \n",
    "Взять страницу с википедии (https://ru.wikipedia.org/wiki/Python) и найти все ссылки, которые с неё ведут.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение стандарными средствами Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/wiki/%D0%92%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D1%8F:%D0%9F%D0%B0%D1%82%D1%80%D1%83%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5'] 1357\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(\"https://ru.wikipedia.org/wiki/Python\").read().decode('utf-8')\n",
    "\n",
    "hrefs = []\n",
    "pos = html.find('<a href=')\n",
    "while pos != -1:\n",
    "    pos2 = html.find('\"', pos + 9)\n",
    "    hrefs.append(html[pos + 9:pos2])\n",
    "    pos = html.find('<a href=', pos2 + 1)\n",
    "print(hrefs[0:1], len(hrefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение с помощью библионтеки Beautiful Soup.\n",
    "\n",
    "Для этого сначала я установила последний на текущий момент (дек. 2020) релиз данной библиотеки в текущее окружение Anaconda с помощью следующей команды: conda install beautifulsoup4.\n",
    "\n",
    "Далее импортирую библиотеку с помощью команды:  \n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing a page with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>A simple example page</title>\n",
      "</head>\n",
      "<body>\n",
      "<p>Here is some simple content for this page.</p>\n",
      "</body>\n",
      "</html> ['\\n', <head>\n",
      "<title>A simple example page</title>\n",
      "</head>, '\\n', <body>\n",
      "<p>Here is some simple content for this page.</p>\n",
      "</body>, '\\n'] <class 'bs4.element.Tag'> <class 'list_iterator'>\n",
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "content = urlopen(\"http://dataquestio.github.io/web-scraping-pages/simple.html\").read().decode('utf-8')\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "#print(content)\n",
    "#print(soup.prettify())\n",
    "\n",
    "#print(list(soup.children), type(soup), type(soup.children))\n",
    "#print([type(item) for item in soup.children])\n",
    "\n",
    "#all_content[1].children # 'NavigableString' object has no attribute 'children'\n",
    "\n",
    "html = list(soup.children)[2]\n",
    "#print(html, list(html.children), type(html), type(html.children))\n",
    "\n",
    "body = list(html.children)[3]\n",
    "#print(list(body.children), type(body))\n",
    "#print([type(item) for item in body.children])\n",
    "\n",
    "p = list(body.children)[1]\n",
    "#print(p, type(p))\n",
    "\n",
    "print(p.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавлю в строку с html-страницей свободный текст, который не содержится внутри каких-либо тегов и посмотрю, что выведит list(soup.children):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "Some text which is not in html tag\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A simple example page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p>\n",
      "   Here is some simple content for this page.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "['html', '\\nSome text which is not in html tag', <html>\n",
      "<head>\n",
      "<title>A simple example page</title>\n",
      "</head>\n",
      "<body>\n",
      "<p>Here is some simple content for this page.</p>\n",
      "</body>\n",
      "</html>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[bs4.element.Doctype, bs4.element.NavigableString, bs4.element.Tag]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = '<!DOCTYPE html>\\nSome text which is not in html tag<html>\\n    <head>\\n        <title>A simple example page</title>\\n    </head>\\n    <body>\\n        <p>Here is some simple content for this page.</p>\\n    </body>\\n</html>'\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "print(soup.prettify())\n",
    "print(list(soup.children))\n",
    "[type(item) for item in soup.children]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all instances of a tag at once\n",
    "What we did above was useful for figuring out how to navigate a page, but it took a lot of commands to do something fairly simple. If we want to extract a single tag, we can instead use the __find_all__ method, which will find all the instances of a tag on a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>Here is some simple content for this page.</p>]\n",
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "content = urlopen(\"http://dataquestio.github.io/web-scraping-pages/simple.html\").read().decode('utf-8')\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "print(soup.find_all('p'))\n",
    "\n",
    "print(soup.find_all('p')[0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you instead only want __to find the first instance of a tag__, you can use the __find__ method, which will return a single BeautifulSoup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Here is some simple content for this page.</p>\n",
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('p'))\n",
    "print(soup.find('p').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for tags by class and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A simple example page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div>\n",
      "   <p class=\"inner-text first-item\" id=\"first\">\n",
      "    First paragraph.\n",
      "   </p>\n",
      "   <p class=\"inner-text\">\n",
      "    Second paragraph.\n",
      "   </p>\n",
      "  </div>\n",
      "  <p class=\"outer-text first-item\" id=\"second\">\n",
      "   <b>\n",
      "    First outer paragraph.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"outer-text\">\n",
      "   <b>\n",
      "    Second outer paragraph.\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "[<p class=\"inner-text first-item\" id=\"first\">\n",
      "                First paragraph.\n",
      "            </p>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "content = urlopen(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\").read().decode('utf-8')\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "print(soup.prettify())\n",
    "\n",
    "# search for any p tag that has the class outer-text:\n",
    "#print(soup.find_all('p', class_='outer-text'))\n",
    "\n",
    "# search for any tag that has the class outer-text:\n",
    "#print(soup.find_all(class_='outer-text'))\n",
    "\n",
    "# search for elements by id:\n",
    "print(soup.find_all(id='first'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CSS Selectors\n",
    "BeautifulSoup objects support searching a page via CSS selectors using the __select__ method. We can use CSS selectors to find all the p tags in our page that are inside of a div like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"inner-text first-item\" id=\"first\">\n",
      "                First paragraph.\n",
      "            </p>, <p class=\"inner-text\">\n",
      "                Second paragraph.\n",
      "            </p>] <class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(soup.select('div p'), type(soup.select('div p')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring page structure with Chrome DevTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"tombstone-container\">\n",
      " <p class=\"period-name\">\n",
      "  This\n",
      "  <br/>\n",
      "  Afternoon\n",
      " </p>\n",
      " <p>\n",
      "  <img alt=\"This Afternoon: Mostly sunny, with a high near 59. North wind around 6 mph becoming light and variable. \" class=\"forecast-icon\" src=\"newimages/medium/sct.png\" title=\"This Afternoon: Mostly sunny, with a high near 59. North wind around 6 mph becoming light and variable. \"/>\n",
      " </p>\n",
      " <p class=\"short-desc\">\n",
      "  Mostly Sunny\n",
      " </p>\n",
      " <p class=\"temp temp-high\">\n",
      "  High: 59 °F\n",
      " </p>\n",
      "</div>\n",
      "\n",
      "ThisAfternoon\n",
      "Mostly Sunny\n",
      "High: 59 °F\n",
      "\n",
      "This Afternoon: Mostly sunny, with a high near 59. North wind around 6 mph becoming light and variable. \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "content = urlopen(\"https://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168#.X9e6sfMzbIV\").read().decode('utf-8')\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "seven_day = soup.find(id=\"seven-day-forecast\")\n",
    "forecast_items = seven_day.find_all(class_=\"tombstone-container\")\n",
    "tonight = forecast_items[1]\n",
    "#print(type(tonight))\n",
    "print(tonight.prettify())\n",
    "\n",
    "period = tonight.find(class_=\"period-name\").get_text()\n",
    "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
    "temp = tonight.find(class_=\"temp\").get_text()\n",
    "print()\n",
    "print(period)\n",
    "print(short_desc)\n",
    "print(temp)\n",
    "\n",
    "img = tonight.find(\"img\")\n",
    "#print(img, type(img))\n",
    "# Treat the BS object like a dictionary, and pass in \n",
    "# the attribute we want as a key:\n",
    "desc = img[\"title\"]\n",
    "print()\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting all the information from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOW: Multiplehazards in effect', 'Today', 'Tonight', 'Tuesday', 'TuesdayNight', 'Wednesday', 'WednesdayNight', 'Thursday', 'ThursdayNight']\n"
     ]
    }
   ],
   "source": [
    "# Select all items with the class period-name inside an item with \n",
    "# the class tombstone-container in seven_day.\n",
    "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
    "# Use a list comprehension to call the get_text method on each BS object\n",
    "periods = [p.get_text() for p in period_tags]\n",
    "print(periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Click HERE for Details', 'Mostly Sunny', 'Partly Cloudy', 'Mostly Sunny', 'Mostly Cloudy', 'Chance Rainthen RainLikely', 'Rain Likely', 'Chance Rainthen MostlySunny', 'Mostly Clear']\n",
      "['High: 59 °F', 'Low: 46 °F', 'High: 59 °F', 'Low: 48 °F', 'High: 60 °F', 'Low: 51 °F', 'High: 58 °F', 'Low: 46 °F']\n",
      "['', 'This Afternoon: Mostly sunny, with a high near 59. North wind around 6 mph becoming light and variable. ', 'Tonight: Partly cloudy, with a low around 46. West wind around 6 mph becoming calm  in the evening. ', 'Tuesday: Mostly sunny, with a high near 59. Calm wind. ', 'Tuesday Night: Mostly cloudy, with a low around 48. Light and variable wind. ', 'Wednesday: Rain likely, mainly after 5pm.  Mostly cloudy, with a high near 60. Calm wind becoming southeast around 5 mph.  Chance of precipitation is 60%. New precipitation amounts of less than a tenth of an inch possible. ', 'Wednesday Night: Rain likely, mainly before 5am.  Cloudy, with a low around 51. Chance of precipitation is 60%. New precipitation amounts between a quarter and half of an inch possible. ', 'Thursday: A 30 percent chance of rain before 11am.  Mostly sunny, with a high near 58.', 'Thursday Night: Mostly clear, with a low around 46.']\n"
     ]
    }
   ],
   "source": [
    "short_descs = [sd.get_text() for sd in seven_day.select(\".tombstone-container .short-desc\")]\n",
    "temps = [t.get_text() for t in seven_day.select(\".tombstone-container .temp\")]\n",
    "descs = [d[\"title\"] for d in seven_day.select(\".tombstone-container img\")]\n",
    "print(short_descs)\n",
    "print(temps)\n",
    "print(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94596065609609271052308\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "resp = urlopen('https://stepik.org/media/attachments/lesson/245130/6.html') # скачиваем файл\n",
    "html = resp.read().decode('utf8') # считываем содержимое\n",
    "soup = BeautifulSoup(html, 'html.parser') # делаем суп\n",
    "table = soup.find('table', attrs = {'class' : 'wikitable sortable'})\n",
    "cnt = 0\n",
    "for tr in soup.find_all('tr'):\n",
    "    cnt += 1\n",
    "    for td in tr.find_all(['td', 'th']):\n",
    "        cnt *= 2\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stepic_web_gust_py3_7",
   "language": "python",
   "name": "stepic_web_gust_py3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
